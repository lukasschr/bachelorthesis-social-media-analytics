{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c0d714",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b5f0f",
   "metadata": {},
   "source": [
    "Inhaltsverzeichnis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4a674bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9543bb",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba3f08e",
   "metadata": {},
   "source": [
    "In der Data Exploration konnte festgestellt werden, dass der Datensatz nicht vollständig den Anforderungen entspricht und einige Fehler aufweist. Im Data Cleaning Prozess wird der Datensatz daher angepasst und verbessert. Identifizierte Fehler werden behoben. Ziel ist es, den Datensatz für die Preprocessing Pipeline vorzubereiten. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a428d5",
   "metadata": {},
   "source": [
    "#### A. Data Cleaning \n",
    "\n",
    "Der nachfolgende Code führt den gesamten Data Cleaning Prozess durch. Alternativ kann das Data Cleaning auch Schritt für Schritt eigenständig durchgeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d20c270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "from src.features.data_cleaning import *\n",
    "\n",
    "# run data cleaning\n",
    "pipeline = CleaningPipeline(path='../data/raw/twitter_tweets_raw.pkl')\n",
    "df = pipeline.run()\n",
    "\n",
    "# save cleaned data set\n",
    "df.to_feather('../data/intermediate/twitter_tweets_intermediate.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe354cd",
   "metadata": {},
   "source": [
    "#### 0. Datensatz laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61f087d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.nitter_scraper_standalone_v2 import Tweet, TweetScraper\n",
    "import pandas as pd\n",
    "\n",
    "# load tweet objects\n",
    "list_of_tweets = TweetScraper.load_collected_tweets(path='../data/raw/twitter_tweets_raw.pkl')\n",
    "\n",
    "# transform into a dataframe\n",
    "dict_of_tweets =  [{\"url\": tweet.url, \"date\": tweet.date, \"rawContent\": tweet.rawContent} for tweet in list_of_tweets]\n",
    "df = pd.DataFrame(dict_of_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48585905",
   "metadata": {},
   "source": [
    "#### 1. Duplikate löschen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12bb96da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['rawContent'], inplace=True)\n",
    "\n",
    "# check for success\n",
    "if df['rawContent'].duplicated().any():\n",
    "    print(f\"{len(df[df['rawContent'].duplicated()])} duplicates found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721db4ed",
   "metadata": {},
   "source": [
    "#### 2. Nicht-englische Beiträge identifizieren & löschen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f84ba184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 854238/854238 [57:31<00:00, 247.47it/s]\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "from tqdm import tqdm\n",
    "\n",
    "# define function to identify language of tweet\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except:\n",
    "        lang = None\n",
    "    return lang\n",
    "\n",
    "tqdm.pandas()\n",
    "# determine the language for each tweet\n",
    "df['lang'] = df['rawContent'].progress_apply(detect_language)\n",
    "\n",
    "# identify and delete all non-english tweets\n",
    "non_english_posts = df.query('lang != \"en\"')\n",
    "df.drop(index=non_english_posts.index, inplace=True)\n",
    "\n",
    "# check for success\n",
    "if not df['lang'].eq('en').all():\n",
    "    print(df.query('lang != \"en\"'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb393a3",
   "metadata": {},
   "source": [
    "#### 3. Date aktualisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c32969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "df['date'] = df['date'].apply(lambda x: datetime.datetime.strptime(x, \"%b %d, %Y · %I:%M %p %Z\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce97780",
   "metadata": {},
   "source": [
    "#### 4. Irrelevante Daten löschen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64015be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['lang'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1e439d",
   "metadata": {},
   "source": [
    "#### 5. Gesäuberten Datensatz speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da93d084",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('url', inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "df.to_feather('../data/intermediate/twitter_tweets_intermediate.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7d54a9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1562e931",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b4b3e",
   "metadata": {},
   "source": [
    "Nachdem der Datensatz gesäubert wurde, werden die Textdaten in eine sogenannte Preprocessing Pipeline gegeben. Die Preprocessing Pipeline ist entscheidend für das spätere Ergebnis des Modelings. Diese stellt sicher, dass der Text für das Modell geeignet ist. Die Pipeline, die für die Anforderungen des Projekts speziell entworfen wurde, kann wie folgt visualisiert werden:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c537c985",
   "metadata": {},
   "source": [
    "TODO: ABBILDUNG VON DER PREPROCESSING PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d363534b",
   "metadata": {},
   "source": [
    "#### A. Preprocessing Pipeline\n",
    "\n",
    "Der nachfolgende Code führt die gesamte Preprocessing Pipeline aus. Alternativ kann die Preprocessing Pipeline auch Schritt für Schritt eigenständig ausgeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c11f958b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 14:29:14,545 - INFO - initialize pipeline and download required nltk packages...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lukas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lukas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lukas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2023-05-23 14:29:14,818 - WARNING - starting default preprocessing pipeline\n",
      "2023-05-23 14:29:14,818 - INFO - remove urls...\n",
      "100%|███████████████████████████████████████████████████████████████████████| 851926/851926 [00:11<00:00, 76401.46it/s]\n",
      "2023-05-23 14:29:26,017 - INFO - remove twitter user mentions...\n",
      "100%|██████████████████████████████████████████████████████████████████████| 851926/851926 [00:02<00:00, 369182.39it/s]\n",
      "2023-05-23 14:29:28,369 - INFO - fix contractions...\n",
      "100%|███████████████████████████████████████████████████████████████████████| 851926/851926 [00:10<00:00, 82098.93it/s]\n",
      "2023-05-23 14:29:38,786 - INFO - tokenize text...\n",
      "100%|███████████████████████████████████████████████████████████████████████| 851926/851926 [00:14<00:00, 58245.00it/s]\n",
      "2023-05-23 14:29:53,530 - INFO - lowercase tokens...\n",
      "100%|███████████████████████████████████████████████████████████████████████| 851926/851926 [00:10<00:00, 79946.81it/s]\n",
      "2023-05-23 14:30:04,881 - INFO - remove punctuation...\n",
      "100%|██████████████████████████████████████████████████████████████████████| 851926/851926 [00:06<00:00, 124422.36it/s]\n",
      "2023-05-23 14:30:12,110 - INFO - remove numeric values...\n",
      "100%|██████████████████████████████████████████████████████████████████████| 851926/851926 [00:04<00:00, 198728.13it/s]\n",
      "2023-05-23 14:30:16,665 - INFO - remove stopwords...\n",
      "100%|███████████████████████████████████████████████████████████████████████| 851926/851926 [00:39<00:00, 21824.41it/s]\n",
      "2023-05-23 14:30:56,090 - INFO - remove emojis...\n",
      "100%|███████████████████████████████████████████████████████████████████████| 851926/851926 [00:17<00:00, 49376.62it/s]\n",
      "2023-05-23 14:31:13,588 - INFO - lemmatize tokens...\n",
      "100%|███████████████████████████████████████████████████████████████████████| 851926/851926 [00:53<00:00, 15828.14it/s]\n",
      "2023-05-23 14:32:07,682 - INFO - preprocessing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "from src.features.preprocessing_pipeline import *\n",
    "import pandas as pd\n",
    "\n",
    "# run preprocessing pipeline\n",
    "pipeline = DefaultPipeline(dataframe=pd.read_feather('../data/intermediate/twitter_tweets_intermediate.feather'))\n",
    "df = pipeline.run()\n",
    "\n",
    "# save preprocessed data set\n",
    "df.to_feather('../data/processed/twitter_tweets_processed.feather')\n",
    "df.to_csv('../data/processed/twitter_tweets_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948f3fb2",
   "metadata": {},
   "source": [
    "#### 0. Packages & Datensatz laden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f82f047c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lukas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lukas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lukas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>rawContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://twitter.com/_Bob_S/status/164159110508...</td>\n",
       "      <td>2023-03-30 23:59:00</td>\n",
       "      <td>Govt IT security isnt 'a nice thing to do': it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://twitter.com/WYSIWYGVentures/status/164...</td>\n",
       "      <td>2023-03-30 23:59:00</td>\n",
       "      <td>ISC West 2023: Cyberattackers Are Targeting Ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://twitter.com/HackerAran7/status/1641591...</td>\n",
       "      <td>2023-03-30 23:59:00</td>\n",
       "      <td>What’s the hack. #stem #science #stemeducation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://twitter.com/bytefeedai/status/16415909...</td>\n",
       "      <td>2023-03-30 23:59:00</td>\n",
       "      <td>BuzzFeed Is Using AI To Write SEO-Bait Travel ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url                date   \n",
       "0  https://twitter.com/_Bob_S/status/164159110508... 2023-03-30 23:59:00  \\\n",
       "1  https://twitter.com/WYSIWYGVentures/status/164... 2023-03-30 23:59:00   \n",
       "2  https://twitter.com/HackerAran7/status/1641591... 2023-03-30 23:59:00   \n",
       "3  https://twitter.com/bytefeedai/status/16415909... 2023-03-30 23:59:00   \n",
       "\n",
       "                                          rawContent  \n",
       "0  Govt IT security isnt 'a nice thing to do': it...  \n",
       "1  ISC West 2023: Cyberattackers Are Targeting Ph...  \n",
       "2  What’s the hack. #stem #science #stemeducation...  \n",
       "3  BuzzFeed Is Using AI To Write SEO-Bait Travel ...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import contractions\n",
    "import nltk\n",
    "import string\n",
    "import emoji\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# download required nltk packages\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# enable progress bar\n",
    "tqdm.pandas()\n",
    "\n",
    "# load dataframe\n",
    "df = pd.read_feather('../data/intermediate/twitter_tweets_intermediate.feather')\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4937c7",
   "metadata": {},
   "source": [
    "#### 1.  URLs entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "edf78af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 851926/851926 [00:12<00:00, 69055.39it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_urls(text):\n",
    "    # define regex pattern for url detection\n",
    "    url_pattern = re.compile(r'\\b(?:https?://)?(?:[a-z]+\\.[a-z]+\\.[a-z]+|[a-z]+\\.[a-z]+(?:/[^\\s]*)?)\\b')\n",
    "    # remove url matches from the text\n",
    "    text_without_urls = re.sub(url_pattern, '', text)\n",
    "    return text_without_urls\n",
    "\n",
    "df['preprocessed_text'] = df['rawContent'].progress_apply(remove_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2ce29d",
   "metadata": {},
   "source": [
    "#### 2.  Erwähnungen entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f526f0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 851926/851926 [00:02<00:00, 342273.36it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_mentions(text):\n",
    "    # define regex pattern for user mentions\n",
    "    mention_pattern = re.compile(r'@\\w+')\n",
    "    # remove user mentions from the text\n",
    "    text_without_mentions = re.sub(mention_pattern, '', text)\n",
    "    return text_without_mentions\n",
    "\n",
    "df['preprocessed_text'] = df['preprocessed_text'].progress_apply(remove_mentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c919f9",
   "metadata": {},
   "source": [
    "#### 3. Kontraktionen auflösen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72986f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 851926/851926 [00:10<00:00, 79503.52it/s]\n"
     ]
    }
   ],
   "source": [
    "def fix_contractions(text):\n",
    "    try:\n",
    "        return contractions.fix(text)\n",
    "    except IndexError: # error should not appear\n",
    "        return text\n",
    "\n",
    "df['preprocessed_text'] = df['preprocessed_text'].progress_apply(fix_contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a08c14d",
   "metadata": {},
   "source": [
    "#### 4. Tokenization durchführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "101c8250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 851926/851926 [00:16<00:00, 53018.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# define tokenizer function\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "df['preprocessed_text'] = df['preprocessed_text'].progress_apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8b8b58",
   "metadata": {},
   "source": [
    "#### 5. Tokens in Kleinbuchstaben umwandeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "678c564d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 851926/851926 [00:12<00:00, 67626.35it/s]\n"
     ]
    }
   ],
   "source": [
    "def lowercase(tokens):\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "df['preprocessed_text'] = df['preprocessed_text'].progress_apply(lowercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acb8f50",
   "metadata": {},
   "source": [
    "#### 6. Satzzeichen entfernen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e904f8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 851926/851926 [00:15<00:00, 56254.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# adding more characters to the punctuation list\n",
    "punct = string.punctuation + \"’\" + \"``\" +\"`\" + \"''\" +\"'\" + \"•\" + \"“\" + \"”\" + \"…\" + \"�\" + \"‘\" + \"…\" + \"/…\" + \"-…\" + \"-#\" + \"’\" + \"...\"\n",
    "\n",
    "def remove_punct(tokens):\n",
    "    return [token for token in tokens if token not in punct]\n",
    "\n",
    "df['preprocessed_text'] = df['preprocessed_text'].progress_apply(remove_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115b3f4a",
   "metadata": {},
   "source": [
    "#### 7.  Numerische Daten entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8605fcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 851926/851926 [00:04<00:00, 202292.88it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_numerics(tokens):\n",
    "    return [token for token in tokens if not token.isdigit()]\n",
    "\n",
    "df['preprocessed_text'] = df['preprocessed_text'].progress_apply(remove_numerics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c66dc0",
   "metadata": {},
   "source": [
    "#### 8.  Stopwörter entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2cdb450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 851926/851926 [00:39<00:00, 21569.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# define list of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stop_words and len(token) > 1]\n",
    "\n",
    "df['preprocessed_text'] = df['preprocessed_text'].progress_apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c0bbf3",
   "metadata": {},
   "source": [
    "#### 9.  Emojis entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f21c182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 851926/851926 [00:18<00:00, 46123.68it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_emoji(tokens):\n",
    "    return [token for token in tokens if not any(char in emoji.EMOJI_DATA for char in token)]\n",
    "\n",
    "df['preprocessed_text'] = df['preprocessed_text'].progress_apply(remove_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66082400",
   "metadata": {},
   "source": [
    "#### 10. Lemmatisierung durchführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4aad7019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 851926/851926 [00:54<00:00, 15540.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# initialization of the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "df['preprocessed_text'] = df['preprocessed_text'].progress_apply(lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12981cc3",
   "metadata": {},
   "source": [
    "#### 11. Preprocessed Datensatz speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f77a4e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_feather('../data/processed/twitter_tweets_processed.feather')\n",
    "df.to_csv('../data/processed/twitter_tweets_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90561981",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
