{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c0d714",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af387cd",
   "metadata": {},
   "source": [
    "Die im Rahmen des Data Understanding gewonnenen Erkenntnisse √ºber die Qualit√§t der Daten werden nun gezielt eingesetzt, um eine Datenbereinigung und Vorverarbeitung f√ºr die Modellierung durchzuf√ºhren. Dadurch wird sichergestellt, dass die Daten qualitativ hochwertig und geeignet f√ºr den Einsatz in den Modellen sind. Dies erm√∂glicht eine robuste und aussagekr√§ftige Analyse sowie eine zuverl√§ssige Modellbildung."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b5f0f",
   "metadata": {},
   "source": [
    "#### Inhalt\n",
    "\n",
    "- [Data Cleaning](#data_cleaning)\n",
    "- [Preprocessing Pipeline](#preprocessing_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136d1e1c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9543bb",
   "metadata": {},
   "source": [
    "## Data Cleaning <a name=\"data_cleaning\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba3f08e",
   "metadata": {},
   "source": [
    "Es wurde festgestellt, dass der vorliegende Datensatz nicht vollst√§ndig den vorgegebenen Anforderungen entspricht und M√§ngel in der Datenqualit√§t aufweist. Um diese M√§ngel zu beheben und den Datensatz zu verbessern, wird nun der Data Cleaning Prozess durchgef√ºhrt. Das √ºbergeordnete Ziel des Data Cleaning besteht darin, den Datensatz so zu bereinigen, dass er f√ºr die nachfolgende Preprocessing Pipeline optimal vorbereitet ist. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a428d5",
   "metadata": {},
   "source": [
    "#### üß© Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d20c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo Skipping Codeblock.. (<- Delete or comment this line to run the code block)\n",
    "from src.features.data_cleaning import *\n",
    "\n",
    "# run data cleaning\n",
    "pipeline = CleaningPipeline(path='../data/raw/twitter_tweets_raw.feather')\n",
    "df = pipeline.run()\n",
    "\n",
    "# save cleaned data set\n",
    "df.to_feather('../data/intermediate/twitter_tweets_intermediate.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe354cd",
   "metadata": {},
   "source": [
    "#### 0. Datensatz laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f087d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_feather('../data/raw/twitter_tweets_raw.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48585905",
   "metadata": {},
   "source": [
    "#### 1. Duplikate l√∂schen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb96da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['rawContent'], inplace=True)\n",
    "\n",
    "# check for success\n",
    "if df['rawContent'].duplicated().any():\n",
    "    print(f\"{len(df[df['rawContent'].duplicated()])} duplicates found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721db4ed",
   "metadata": {},
   "source": [
    "#### 2. Nicht-englische Beitr√§ge identifizieren & l√∂schen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ba184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import logger\n",
    "\n",
    "non_english_posts = df.query('lang != \"en\"')\n",
    "logger.info(f'{len(non_english_posts)} non-English tweets found! drop...')\n",
    "df.drop(index=non_english_posts.index, inplace=True)\n",
    "\n",
    "# check for success\n",
    "if not df['lang'].eq('en').all():\n",
    "    print(df.query('lang != \"en\"'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb393a3",
   "metadata": {},
   "source": [
    "#### 3. Date aktualisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce97780",
   "metadata": {},
   "source": [
    "#### 4. Irrelevante Daten l√∂schen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64015be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['lang', 'replyCount', 'retweetCount', 'likeCount'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1e439d",
   "metadata": {},
   "source": [
    "#### 5. Ges√§uberten Datensatz speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da93d084",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('url', inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "df.to_feather('../data/intermediate/twitter_tweets_intermediate.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7d54a9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1562e931",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline <a name=\"preprocessing_pipeline\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b4b3e",
   "metadata": {},
   "source": [
    "Nachfolgend werden die enthaltenen Textdaten einer speziell entwickelten Preprocessing Pipeline zugef√ºhrt. Die Durchf√ºhrung der Preprocessing Pipeline hat einen entscheidenden Einfluss auf die Qualit√§t und das sp√§tere Ergebnis der Modellierungen. Die Hauptaufgabe der Preprocessing Pipeline besteht darin, sicherzustellen, dass die Textdaten f√ºr die Modellierung geeignet und optimal vorbereitet sind. Die speziell f√ºr die Anforderungen dieses Projekts entworfene Pipeline kann visuell wie folgt dargestellt werden:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c537c985",
   "metadata": {},
   "source": [
    "<center><img src=\"../export/preprocessing_pipeline.png\" alt=\"Preprocessing Pipeline\" style=\"width: 75%;\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d363534b",
   "metadata": {},
   "source": [
    "#### üß© Preprocessing Pipeline ‚òï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11f958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo Skipping Codeblock.. (<- Delete or comment this line to run the code block)\n",
    "from src.features.preprocessing_pipeline import *\n",
    "import pandas as pd\n",
    "\n",
    "# run preprocessing pipeline\n",
    "pipeline = DefaultPipeline(dataframe=pd.read_feather('../data/intermediate/twitter_tweets_intermediate.feather'))\n",
    "df = pipeline.run()\n",
    "\n",
    "# save preprocessed data set\n",
    "df.to_feather('../data/processed/twitter_tweets_processed_2.feather')\n",
    "df.to_csv('../data/processed/twitter_tweets_processed_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948f3fb2",
   "metadata": {},
   "source": [
    "#### 0. Packages & Datensatz laden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import contractions\n",
    "import nltk\n",
    "import string\n",
    "import emoji\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# download required nltk packages\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# enable progress bar\n",
    "tqdm.pandas()\n",
    "\n",
    "# load dataframe\n",
    "df = pd.read_feather('../data/intermediate/twitter_tweets_intermediate.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4937c7",
   "metadata": {},
   "source": [
    "#### 1.  URLs entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf78af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    # define regex pattern for url detection\n",
    "    url_pattern = re.compile(r'\\b(?:https?://)?(?:[a-z]+\\.[a-z]+\\.[a-z]+|[a-z]+\\.[a-z]+(?:/[^\\s]*)?)\\b')\n",
    "    # remove url matches from the text\n",
    "    text_without_urls = re.sub(url_pattern, '', text)\n",
    "    return text_without_urls\n",
    "\n",
    "df['preprocessed_text'] = df['rawContent'].progress_apply(remove_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2ce29d",
   "metadata": {},
   "source": [
    "#### 2.  Erw√§hnungen entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f526f0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(text):\n",
    "    # define regex pattern for user mentions\n",
    "    mention_pattern = re.compile(r'@\\w+')\n",
    "    # remove user mentions from the text\n",
    "    text_without_mentions = re.sub(mention_pattern, '', text)\n",
    "    return text_without_mentions\n",
    "\n",
    "df['preprocessed_text'] = df['preprocessed_text'].progress_apply(remove_mentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c919f9",
   "metadata": {},
   "source": [
    "#### 3. Kontraktionen aufl√∂sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72986f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_contractions(text):\n",
    "    try:\n",
    "        return contractions.fix(text)\n",
    "    except IndexError: # error should not appear\n",
    "        return text\n",
    "\n",
    "df['preprocessed_text'] = df['preprocessed_text'].progress_apply(fix_contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a08c14d",
   "metadata": {},
   "source": [
    "#### 4. Tokenization durchf√ºhren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101c8250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define tokenizer function\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "df['preprocessed_text'] = df['preprocessed_text'].progress_apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8b8b58",
   "metadata": {},
   "source": [
    "#### 5. Tokens in Kleinbuchstaben umwandeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678c564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(tokens):\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "df['preprocessed_text'] = df['preprocessed_text'].progress_apply(lowercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acb8f50",
   "metadata": {},
   "source": [
    "#### 6. Satzzeichen entfernen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e904f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding more characters to the punctuation list\n",
    "punct = string.punctuation + \"‚Äô\" + \"``\" +\"`\" + \"''\" +\"'\" + \"‚Ä¢\" + \"‚Äú\" + \"‚Äù\" + \"‚Ä¶\" + \"ÔøΩ\" + \"‚Äò\" + \"‚Ä¶\" + \"/‚Ä¶\" + \"-‚Ä¶\" + \"-#\" + \"‚Äô\" + \"...\" + \".‚Äù\" + \"!!\"\n",
    "\n",
    "def remove_punct(tokens):\n",
    "    return [token for token in tokens if token not in punct]\n",
    "\n",
    "df['preprocessed_text'] = df['preprocessed_text'].progress_apply(remove_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115b3f4a",
   "metadata": {},
   "source": [
    "#### 7.  Numerische Daten entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8605fcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numerics(tokens):\n",
    "    return [token for token in tokens if not token.isdigit()]\n",
    "\n",
    "df['preprocessed_text'] = df['preprocessed_text'].progress_apply(remove_numerics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c66dc0",
   "metadata": {},
   "source": [
    "#### 8.  Stopw√∂rter entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cdb450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "additional_stop_words = ['u']\n",
    "stop_words.extend(additional_stop_words)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stop_words and len(token) > 1]\n",
    "\n",
    "df['preprocessed_text'] = df['preprocessed_text'].progress_apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c0bbf3",
   "metadata": {},
   "source": [
    "#### 9.  Emojis entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f21c182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(tokens):\n",
    "    return [token for token in tokens if not any(char in emoji.EMOJI_DATA for char in token)]\n",
    "\n",
    "df['preprocessed_text'] = df['preprocessed_text'].progress_apply(remove_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66082400",
   "metadata": {},
   "source": [
    "#### 10. Lemmatisierung durchf√ºhren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aad7019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization of the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "df['preprocessed_text'] = df['preprocessed_text'].progress_apply(lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12981cc3",
   "metadata": {},
   "source": [
    "#### 11. Preprocessed Datensatz speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77a4e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_feather('../data/processed/twitter_tweets_processed.feather')\n",
    "df.to_csv('../data/processed/twitter_tweets_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90561981",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
