{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "885611ce",
   "metadata": {},
   "source": [
    "# Modeling & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45364137",
   "metadata": {},
   "source": [
    "Nachdem die Daten einer optimalen Vorbereitung unterzogen wurden, erfolgt nun die Berechnung, Evaluation und Optimierung der Modelle. Hierbei kommen geeignete Modellierungsverfahren zum Einsatz, die den Anforderungen des Forschungsprojekts gerecht werden. Nachdem die Modelle berechnet wurden, erfolgt eine gr√ºndliche Evaluation ihrer Leistungen. Hierbei werden verschiedene Evaluationsmetriken herangezogen, um die Vorhersagegenauigkeit, Robustheit und andere relevante Aspekte zu bewerten. Auf Basis der Evaluationsergebnisse werden die Modelle weiter optimiert, um ihre Leistung zu verbessern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49bd967",
   "metadata": {},
   "source": [
    "#### Inhalt\n",
    "\n",
    "- [Topic Modeling](#topic_modeling)\n",
    "- [Time Series Forecasting](#time_series_forecasting)\n",
    "- [Exkurs: Sentiment-Analyse zur Identifizierung von Polarisierungsph√§nomen](#polarization_phenomenon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a1418a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c310be",
   "metadata": {},
   "source": [
    "## Topic Modeling <a name=\"topic_modeling\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a57155",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972a2ec",
   "metadata": {},
   "source": [
    "#### üß© Modeling ‚òï\n",
    "\n",
    "Der folgende Code importiert eigens speziell entwickelte Klassen und Methoden, die f√ºr das LDA-Topic-Modeling optimiert sind. Es wird empfohlen, diese Methode zur Erstellung der Modelle zu verwenden. Eine ausf√ºhrlichere Darstellung und Dokumentation des Codes befindet sich in der Datei `src/models/topic_modeling.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f46028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.models import topic_modeling as tm\n",
    "from src.utils import safe_as_pkl\n",
    "import pandas as pd\n",
    "\n",
    "# load dataframe\n",
    "df = pd.read_feather('../data/processed/twitter_tweets_processed.feather')\n",
    "\n",
    "# create & build lda model\n",
    "lda_model = tm.LdaModel(text=df['preprocessed_text'])\n",
    "lda_model.build(num_topics=11)\n",
    "\n",
    "# export\n",
    "safe_as_pkl(lda_model, path='../models/lda_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbfc2fa",
   "metadata": {},
   "source": [
    "#### Ergebnisse visualisieren ‚òï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9283cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "from src.utils import load_pkl\n",
    "\n",
    "lda_model = load_pkl('../models/lda_model.pkl')\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model.model, lda_model.corpus, lda_model.dictionary, sort_topics=False)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d3abd4",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22df220",
   "metadata": {},
   "source": [
    "#### 1. Coherence Score berechnen ‚òï\n",
    "\n",
    "Der Coherence Score ist ein Evaluationsma√ü f√ºr Topic Models, das versucht, die Koh√§renz der gefundenen Themen zu bewerten. Die Koh√§renz bezieht sich darauf, wie gut die W√∂rter innerhalb eines Themas zusammenpassen und ob sie eine sinnvolle Bedeutung ergeben. Ein hohes Ma√ü an Koh√§renz zeigt an, dass die Themen gut definiert und interpretierbar sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d870e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import topic_modeling as tm\n",
    "from src.utils import load_pkl\n",
    "\n",
    "lda_model = load_pkl('../models/lda_model.pkl')\n",
    "coherence_score = tm.evaluate(model=lda_model.model, text=lda_model.text, dictionary=lda_model.dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249f6e85",
   "metadata": {},
   "source": [
    "#### 2. Hyperparameter Tuning durchf√ºhren üåí"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941582e3",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning ist ein wichtiger Schritt im Machine Learning, der dazu beitr√§gt, das bestm√∂gliche Modell zu finden. In diesem Fall wird das bestm√∂gliche Modell anhand des Coherence Scores bemessen. Ziel des Hyperparameter Tuning ist es demnach, das Modell mit dem h√∂chsten Coherence Score zu finden. Folgende Parameter sollen optimiert werden:\n",
    "\n",
    "`num_topics`, `alpha`, `eta`, `chunksize`, `iterations`, `passes`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345c69a0",
   "metadata": {},
   "source": [
    "*Wertebereiche f√ºr Hyperparameter festlegen*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21bb2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "search_space = {\n",
    "    'num_topics': hp.choice('num_topics', [i for i in range(12, 28)]),\n",
    "    'alpha': hp.choice('alpha', ['symmetric', 'asymmetric', 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n",
    "    'eta': hp.choice('eta', ['symmetric', 'auto', 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n",
    "    'chunksize':  hp.choice('chunksize', [5000]),\n",
    "    'iterations': hp.choice('iterations', [50, 100]),\n",
    "    'passes': hp.choice('passes', [8, 9, 10])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794dd105",
   "metadata": {},
   "source": [
    "*Bayesian Optimization durchf√ºhren* ‚≠ïÔ∏è\n",
    "\n",
    "Die bayesianische Optimierung ist ein Ansatz zur Suche nach den optimalen Parametereinstellungen eines Modells durch die Kombination von Modellierung der Zielfunktion mittels einer Wahrscheinlichkeitsverteilung und adaptiver Exploration des Suchraums.\n",
    "\n",
    "*Hinweis: Um die erforderlichen Berechnungen erfolgreich durchzuf√ºhren, ist eine leistungsstarke Computerumgebung erforderlich. Die nachfolgenden Berechnungen wurden auf einem virtuellen Server mit den folgenden Spezifikationen durchgef√ºhrt*\n",
    "\n",
    "- *Virtualisierungstechnik: KVM*\n",
    "- *Prozessor: 18 vCores*\n",
    "- *RAM: 48GB DDR4 ECC*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664ab4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "from src.models.bayesian_optimization import optimize_topic_modeling\n",
    "\n",
    "df, optimized_parameters = optimize_topic_modeling('../data/processed/twitter_tweets_processed.feather',search_space, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef47581",
   "metadata": {},
   "source": [
    "*Hyperparameter Tuning auswerten*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb45dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_feather('../data/modeling/tm_ht_results.feather')\n",
    "df.sort_values('coherence_score', ascending=False, inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3be0b3",
   "metadata": {},
   "source": [
    "#### 3. Erstellen optimiertes LDA-Modell ‚≠ïÔ∏è‚òï\n",
    "\n",
    "Nach der Ausf√ºhrung des Hyperparameter-Tunings, kann eine gute Auswahl von Hyperparametern getroffen werden. Mithilfe der optimierten Parameter kann im folgenden das optimierte LDA-Modell erstellt und berechnet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34b6d02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "from src.models import topic_modeling as tm\n",
    "from src.utils import safe_as_pkl\n",
    "import pandas as pd\n",
    "\n",
    "# load dataframe\n",
    "df = pd.read_feather('../data/processed/twitter_tweets_processed.feather')\n",
    "\n",
    "# create & build optimized lda model\n",
    "lda_model = tm.LdaMulticoreModel(text=df['preprocessed_text'])\n",
    "lda_model.build(\n",
    "    \n",
    "    seed=1688143687, \n",
    "    num_topics=20, \n",
    "    alpha='asymmetric', \n",
    "    eta=0.3,\n",
    "    chunksize=5000,\n",
    "    iterations=100,\n",
    "    passes=10\n",
    "\n",
    ")\n",
    "\n",
    "# export\n",
    "safe_as_pkl(lda_model, path='../models/optimized_lda_model_174.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e65a56d",
   "metadata": {},
   "source": [
    "#### 4. Ergebnisse visualisieren ‚òï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba6eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "from src.utils import load_pkl\n",
    "\n",
    "# load optimized lda model\n",
    "lda_model = load_pkl('../models/optimized_lda_model_174.pkl')\n",
    "\n",
    "# visualize\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model.model, lda_model.corpus, lda_model.dictionary, sort_topics=False)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e8e79",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cf58e6",
   "metadata": {},
   "source": [
    "## Time Series Forecasting <a name=\"time_series_forecasting\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5998522e",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Im Rahmen der Berechnung der Time Series Forecasting Modelle ist es notwendig, die Daten einer Transformation zu unterziehen. Dieser Transformationsprozess beinhaltet zun√§chst die Zuordnung der Tweets zu den zuvor identifizierten Topics. Dadurch entsteht ein neuer Datensatz, der nach den Topics gruppiert wird. F√ºr jedes Topic wird daraufhin eine spezifische Zeitreihe erstellt. Die Erstellung der Zeitreihen erfolgt durch Z√§hlen der Anzahl von Tweets pro Tag, die einem bestimmten Topic zugeordnet werden k√∂nnen. Diese Vorgehensweise erm√∂glicht es, die zeitliche Entwicklung der Tweets zu analysieren und die relevanten Informationen f√ºr die einzelnen Topics zu erfassen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e76c5cd",
   "metadata": {},
   "source": [
    "#### 1. Topic Zuordnung ‚òï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c109754",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "from src.utils import load_pkl, tweet_topic_assignment\n",
    "import pandas as pd\n",
    "\n",
    "lda_model = load_pkl('../models/optimized_lda_model_174.pkl')\n",
    "df = pd.read_feather('../data/processed/twitter_tweets_processed.feather')\n",
    "\n",
    "df['topics'] = tweet_topic_assignment(lda_model, topic_minimum_probability=0.20)\n",
    "df.to_feather('../data/modeling/topic_assigned_twitter_tweets.feather')\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fbc22c",
   "metadata": {},
   "source": [
    "#### 2. Topic Time Series erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d048d1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.time_series_forecasting import process_to_timeseries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "df_topics_assigned = pd.read_feather('../data/modeling/topic_assigned_twitter_tweets.feather')\n",
    "df_topic_grouped_ts = process_to_timeseries(df_topics_assigned)\n",
    "\n",
    "# define min max scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 100))\n",
    "\n",
    "list_topic_time_series = []\n",
    "for topic, df in df_topic_grouped_ts:\n",
    "    df.drop('topic', axis=1, inplace=True)\n",
    "\n",
    "    # normalize data and convert the normalized data back into a DataFrame\n",
    "    normalized_data = scaler.fit_transform(df)\n",
    "    normalized_df = pd.DataFrame(normalized_data, index=df.index, columns=df.columns)\n",
    "    \n",
    "    ttsd = {'id': (topic+1), 'label': None, 'data': normalized_df}\n",
    "\n",
    "    list_topic_time_series.append(ttsd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b5c29e",
   "metadata": {},
   "source": [
    "#### 3. Labeling\n",
    "\n",
    "*Hinweis: Es ist zu beachten, dass alle nicht gelabelten Topics im Datenbestand aussortiert werden und f√ºr die weiteren Modellierungsprozesse nicht weiter ber√ºcksichtigt werden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ed1aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = {\n",
    "    4: 'Blockchain-Technologie',\n",
    "    10: 'K√ºnstliche Intelligenz',\n",
    "    12: 'VR, AR und Metaverse'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693c3d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "2from src.models.time_series_forecasting import XGBoostModel2\n",
    "from src.utils import safe_as_pkl\n",
    "\n",
    "for ttsd in list_topic_time_series:\n",
    "    if ttsd['id'] in topic_labels:\n",
    "        xgb_model = XGBoostModel2(id=ttsd['id'], timeseries=ttsd['data'])\n",
    "        xgb_model.label = topic_labels[ttsd['id']]\n",
    "        safe_as_pkl(xgb_model, f\"../models/xgb_model_{ttsd['id']}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6ef2c3",
   "metadata": {},
   "source": [
    "#### 4. Aufteilung in Trainings- und Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccf6e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.time_series_forecasting import XGBoostModel2\n",
    "from src.utils import load_pkl, safe_as_pkl\n",
    "\n",
    "xgb_models = [\n",
    "    load_pkl('../models/xgb_model_4.pkl'),\n",
    "    load_pkl('../models/xgb_model_10.pkl'),\n",
    "    load_pkl('../models/xgb_model_12.pkl')\n",
    "]\n",
    "\n",
    "for model in xgb_models:\n",
    "    data_train, data_test = XGBoostModel2.train_test_split(data=model.timeseries, train_size=0.9)\n",
    "    model.data_train = data_train\n",
    "    model.data_test = data_test\n",
    "    safe_as_pkl(model, f\"../models/xgb_model_{model.id}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebc0793",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Feature Engineering bezeichnet den Prozess der Erzeugung neuer Merkmale oder der Transformation vorhandener Merkmale, um die Leistung von Modellen in maschinellem Lernen zu verbessern. Das Feature Engineering erm√∂glicht es dem XGBoost-Modell daher, ein umfassenderes Verst√§ndnis der Daten zu entwickeln und eine verbesserte Vorhersageleistung zu erzielen. Indem relevante Informationen in den Merkmalen gezielt hervorgehoben oder hinzugef√ºgt werden, k√∂nnen nicht-lineare Zusammenh√§nge besser erfasst und die F√§higkeit des Modells zur Generalisierung gesteigert werden.\n",
    "\n",
    "*Hinweis: Das Feature Engineering wird automatisiert bei der Berechnung der Modelle durchgef√ºhrt. Folgende Features wurden gew√§hlt:*\n",
    "\n",
    "```\n",
    "['day'] = data.index.day\n",
    "['week'] = data.index.isocalendar().week.astype(int)\n",
    "['month'] = data.index.month\n",
    "['weekday'] = data.index.weekday\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e98556",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38da2dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.time_series_forecasting import XGBoostModel2\n",
    "from src.utils import load_pkl, safe_as_pkl\n",
    "\n",
    "xgb_models = [\n",
    "    load_pkl('../models/xgb_model_4.pkl'),\n",
    "    load_pkl('../models/xgb_model_10.pkl'),\n",
    "    load_pkl('../models/xgb_model_12.pkl')\n",
    "]\n",
    "\n",
    "for model in xgb_models:\n",
    "    model.build(**{'n_estimators': 1000})\n",
    "    safe_as_pkl(model, f\"../models/xgb_model_{model.id}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a759d56",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2dc52a",
   "metadata": {},
   "source": [
    "#### 1. Hyperparameter Tuning durchf√ºhren üé¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca4b0d2",
   "metadata": {},
   "source": [
    "Auch f√ºr die XGBoost Modelle ist es von Bedeutung ein Hyperparameter Tuning durchzuf√ºhren, um die Genauigkeit der Modelle zu verbessern. Hierbei wird das bestm√∂gliche Modell anhand der Evaluationsmetrik des Mean Absolute Error (MAE) beurteilt. Das Ziel des Hyperparameter Tunings besteht darin, jene Modelle zu ermitteln, das den geringsten durchschnittlichen absoluten Fehler aufweist. Folgende Parameter sollen optimiert werden:\n",
    "\n",
    "`n_estimators`, `learning_rate`, `max_depth`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634cba1a",
   "metadata": {},
   "source": [
    "*Wertebereiche f√ºr Hyperparameter festlegen*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd94a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "import numpy as np\n",
    "\n",
    "search_space = {\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 500, 1000, 1200, 1400, 1600, 1800, 2000, 4000]),\n",
    "    'learning_rate': hp.choice('learning_rate', np.arange(0.01, 0.4, 0.01).tolist()), \n",
    "    'max_depth': hp.choice('max_depth', [i for i in range(5, 73)])        \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fc7ea6",
   "metadata": {},
   "source": [
    "*Bayesian Optimization durchf√ºhren*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9c4762",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.models.time_series_forecasting import XGBoostModel2\n",
    "from src.models.bayesian_optimization import optimize_xgb_modeling\n",
    "from src.utils import safe_as_pkl\n",
    "\n",
    "xgb_models = [\n",
    "    load_pkl('../models/xgb_model_4.pkl'),\n",
    "    load_pkl('../models/xgb_model_10.pkl'),\n",
    "    load_pkl('../models/xgb_model_12.pkl')\n",
    "]\n",
    "\n",
    "for model in xgb_models:\n",
    "    optimized_parameters = optimize_xgb_modeling(model, search_space, 300)\n",
    "    model.build(**optimized_parameters)\n",
    "    safe_as_pkl(model, f\"../models/xgb_model_{model.id}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7ce046",
   "metadata": {},
   "source": [
    "#### 2. Ergebnisse visualisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6d931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.time_series_forecasting import XGBoostModel2\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from src.utils import load_pkl\n",
    "import pandas as pd\n",
    "\n",
    "xgb_models = [\n",
    "    load_pkl('../models/xgb_model_4.pkl'),\n",
    "    load_pkl('../models/xgb_model_10.pkl'),\n",
    "    load_pkl('../models/xgb_model_12.pkl')\n",
    "]\n",
    "\n",
    "for model in xgb_models:\n",
    "    data_test_assigned_predictions = model.data_test.assign(predictions=model.predictions)\n",
    "#     _ = pd.concat([model.data_train, data_test_assigned_predictions])\n",
    "\n",
    "    plt.rcParams['font.family'] = 'Consolas'\n",
    "    plt.figure(figsize=(8, 5)) # width, height\n",
    "    plt.plot(data_test_assigned_predictions['count'], label='data_test', color='#1d3557')\n",
    "    plt.plot(data_test_assigned_predictions['predictions'], label='predictions', color='#ffb703')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'../export/xgb_lchart_{model.id}.svg', format='svg')\n",
    "    \n",
    "    avg = model.data_test['count'].mean()\n",
    "    mae = model.evaluate()\n",
    "    mape = mean_absolute_percentage_error(model.data_test['count'], data_test_assigned_predictions['predictions'])\n",
    "    rmse = mean_squared_error(model.data_test['count'], data_test_assigned_predictions['predictions'], squared=False)\n",
    "    \n",
    "    print(model.label)\n",
    "    print(f'Average value of the dependent variable (AVG): {round(avg, 4)}')\n",
    "    print(f'Mean Absolute Error (MAE): {mae}')\n",
    "    print(f'Mean Absolute Percentage Error (MAPE): {mape}')\n",
    "    print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "    \n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bc182d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cfd04f",
   "metadata": {},
   "source": [
    "## Exkurs: Sentiment-Analyse zur Identifizierung von Polarisierungsph√§nomen <a name=\"polarization_phenomenon\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408167e9",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Im Rahmen des Exkurses wird eine Sentiment-Analyse anhand ausgew√§hlter Twitter-Beitr√§gen durchgef√ºhrt. Hierzu  bedarf es einer Auswahl und Anpassung des zuvor erstellen Datensatzes `topic_assigned_twitter_tweets`. Zun√§chst wird der Trend und der Zeitraum bestimmt, √ºber den die Sentiment-Analyse der Textdaten durchgef√ºhrt werden soll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c1dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection trend and period\n",
    "topic = 12\n",
    "start_date, end_date = '2020-08-01', '2021-02-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4c4a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def to_string(preprocessed_text):\n",
    "    return ' '.join(preprocessed_text)\n",
    "\n",
    "df_topics_assigned = pd.read_feather('../data/modeling/topic_assigned_twitter_tweets.feather')\n",
    "df_topics_assigned.dropna(inplace=True)\n",
    "df_sa = df_topics_assigned[df_topics_assigned['date'].between(start_date, end_date) & df_topics_assigned['topics'].apply(lambda x: (topic-1) in x)]\n",
    "\n",
    "df_sa = df_sa.copy()\n",
    "df_sa['preprocessed_text'] = df_sa['preprocessed_text'].apply(to_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf20708",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "Im nachfolgenden Code wird der Sentiment-Score f√ºr die Textdaten aus dem vorbereiteten Datensatz berechnet. Hierbei wird eine *w√∂rterbuchbasierte* Sentiment-Analyse durchgef√ºhrt, um die positive oder negative Tendenz der einzelnen Textnachrichten zu ermitteln. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ae1e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def calculate_sentiment_score(preprocessed_text):\n",
    "    return analyzer.polarity_scores(preprocessed_text)['compound']\n",
    "\n",
    "df_sa['sentiment_score'] = df_sa['preprocessed_text'].apply(calculate_sentiment_score)\n",
    "\n",
    "print(f\"Sentiment-Score Topic {topic} ({start_date} - {end_date}): {df_sa['sentiment_score'].mean().round(4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f41480-a69d-495b-b652-d104749c263b",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
